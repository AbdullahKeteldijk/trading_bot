{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srema\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import ftplib\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    from requests_html import HTMLSession\n",
    "except Exception:\n",
    "    print(\"\"\"Warning - Certain functionality \n",
    "             requires requests_html, which is not installed.\n",
    "             \n",
    "             Install using: \n",
    "             pip install requests_html\n",
    "             \n",
    "             After installation, you may have to restart your Python session.\"\"\")\n",
    "    \n",
    "import csv\n",
    "import pandas as pd\n",
    "from progressbar import progressbar\n",
    "\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML cleaner\n",
    "# Source: https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python/925630#925630\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    ''' Strip HTML tags from text'''\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data and saving as JSON (new site)\n",
    "\n",
    "def get_html_text(url):\n",
    "    ''' Send HTML request and return HTML text in UTF-8 format'''\n",
    "    \n",
    "    session = HTMLSession()\n",
    "    resp = session.get(url)\n",
    "    text = resp.html.raw_html.decode(\"utf-8\")  \n",
    "    session.close()   \n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    ''' Remove emojis and special characters'''\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    no_emoji = emoji_pattern.sub(r'', text) \n",
    "    return (no_emoji.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "\n",
    "def parse_html(url):\n",
    "    ''' Fetch HTML and clean it  to get raw JSON string'''\n",
    "    \n",
    "    text = get_html_text(url)\n",
    "    text_split = text.split('\\n')\n",
    "    test_str = strip_tags(text_split[-21])\n",
    "    raw_data = test_str[test_str.find('{\"props\"'):]\n",
    "    cleaned_raw_data = remove_emojis(raw_data)\n",
    "    \n",
    "    return cleaned_raw_data\n",
    "\n",
    "def save_json(path, filename, text):\n",
    "    ''' Save scraped text as JSON in UTF-8 format'''\n",
    "    \n",
    "    text_file = codecs.open(f\"{path}/{filename}.json\", \"w\", \"utf-8\")\n",
    "    n = text_file.write(text)\n",
    "    text_file.close()\n",
    "    \n",
    "def get_filename(url):\n",
    "    ''' Get filename from URL'''\n",
    "    year_month = url.split('/')\n",
    "    return year_month[4][:6]\n",
    "    \n",
    "def scrape_json(json_path, url_path):\n",
    "    ''' Scrape URL and save it as JSON'''\n",
    "    \n",
    "    url_list = open_csv(url_path)\n",
    "    \n",
    "    for url in progressbar(url_list):\n",
    "        time.sleep(10)\n",
    "        raw_data = parse_html(url)\n",
    "        filename = get_filename(url)\n",
    "        save_json(json_path, filename, raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing data from JSON files (new site)\n",
    "\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    ''' Flatten dictionary'''\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def clean_data(df):\n",
    "    ''' Adds extra colums to dataframe and renames some columns'''\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    df.columns = df.columns.str.strip('quote')\n",
    "    df_clean['Ticker'] = df_clean['symbol'] + '-USD'\n",
    "    df_clean['Date'] = pd.to_datetime(df_clean['quoteusdlastupdated']).dt.date\n",
    "    df_clean = df_clean.rename(columns={\"id\": \"ID\", \"nam\": \"Name\", \"quoteusdmarketcap\": \"Market_cap\"})\n",
    "    \n",
    "    \n",
    "    return df_clean \n",
    "\n",
    "def remove_list_from_dict(dictionary):\n",
    "    ''' Remove columns from dictionary that are not in the list'''\n",
    "    \n",
    "    key_list = ['id', 'name', 'symbol', 'slug', 'cmcrank', 'marketpaircount', 'circulatingsupply',\n",
    "                'totalsupply', 'lastupdated', 'dateadded', 'tags', 'rank',  \n",
    "                'quoteusdname', 'quoteusdprice', 'quoteusdvolume24h', \n",
    "                'quoteusdmarketcap', 'quoteusdpercentchange1h', 'quoteusdpercentchange24h', \n",
    "                'quoteusdpercentchange7d', 'quoteusdlastupdated']\n",
    "    \n",
    "    copy_dict = dictionary.copy()\n",
    "    \n",
    "    for key in dictionary.keys():\n",
    "        if key not in key_list:\n",
    "            del copy_dict[key]\n",
    "    \n",
    "    return copy_dict\n",
    "\n",
    "def format_keys(dictionary):\n",
    "    ''' Convert keys to lowercase and remove underscore'''\n",
    "    \n",
    "    old_keys = dictionary.keys()\n",
    "    new_dict = {}\n",
    "    \n",
    "    for key, value in dictionary.items():\n",
    "        new_key = key.lower().replace(\"_\", \"\")\n",
    "        new_dict[new_key] = value\n",
    "    \n",
    "    return new_dict\n",
    "    \n",
    "def parse_json(file_path):\n",
    "    ''' Parses JSON file and converts it into a dataframe. \n",
    "    \n",
    "        The JSON structure is loaded as a dictionary and then flatted \n",
    "        after which the data parsed into a dataframe\n",
    "        \n",
    "        params: \n",
    "            file_path: string - Directory if JSON files\n",
    "        \n",
    "        returns:\n",
    "            df_clean: pandas - Pandas dataframe with cleaned data\n",
    "    '''\n",
    "    \n",
    "    row_values = []\n",
    "     \n",
    "    col_names = []\n",
    "    \n",
    "    with open(file_path) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    json_dict = json_data['props']['initialState']['cryptocurrency']['listingLatest']['data']\n",
    "\n",
    "    for i in range(len(json_dict)):\n",
    "        \n",
    "        flat_dict = format_keys(flatten(json_dict[i], parent_key='', sep='_'))\n",
    "        clean_dict = remove_list_from_dict(flat_dict)\n",
    "        item_list = list(clean_dict.values())\n",
    "\n",
    "        row_values.append(item_list)\n",
    "        \n",
    "        if len(col_names) == 0:\n",
    "            col_names = list(clean_dict.keys())\n",
    "    \n",
    "\n",
    "    df_raw = pd.DataFrame(row_values, columns=col_names)\n",
    "                \n",
    "    df_clean = clean_data(df_raw)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and parsing the old site\n",
    "\n",
    "def get_date(url):\n",
    "    ''' Return date from URL'''\n",
    "    \n",
    "    split_url = url.split('/')\n",
    "    raw_date = split_url[4][:8]\n",
    "    return f\"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:]}\"\n",
    "    \n",
    "def list_to_dataframe(raw_data, url, columns):\n",
    "    ''' Convert list of lists with scraped data to a dataframe and add some extra columns'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame.from_records(raw_data, columns=columns)\n",
    "    df['Ticker'] = df['symbol'] + '-USD'\n",
    "    df['Date'] = get_date(url)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def find_url(path, filename):\n",
    "    ''' Find the URL of the file that could not be parsed by looking for a substring in the URL list'''\n",
    "    \n",
    "    url_list = open_csv(path)\n",
    "    date = filename.replace(\".json\", \"\")\n",
    "    index = [i for i, s in enumerate(url_list) if date in s][0]\n",
    "    \n",
    "    return url_list[index]\n",
    "\n",
    "def text_cleaner(text):\n",
    "    ''' Remove duplicates from text'''\n",
    "    \n",
    "    clean_text = text\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        is_int = False\n",
    "        try:\n",
    "            int(text[i])\n",
    "            is_int = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if is_int:\n",
    "            unique_item = text[i+1]\n",
    "            if unique_item == text[i+6]:\n",
    "                del text[i+6]\n",
    "    \n",
    "    return clean_text\n",
    "    \n",
    "def parse_old_site_2016(url_path, filename):\n",
    "    ''' Scrape and parse the old site and return a pandas dataframe\n",
    "    \n",
    "        This funtion is triggered if parsing the JSON file from the \"scrape_json\" could not be parsed.\n",
    "        The function first scrapes the intended page, then removes all the HTML so that \n",
    "        only a list with the data remains. This list is then index and written to a dataframe.\n",
    "        \n",
    "        params:\n",
    "            url_path: string - Directory of the URL list\n",
    "            filename: string - Name of the JSON file that could not be parsed\n",
    "        \n",
    "        returns:\n",
    "            df: pandas dataframe - dataframe with scraped data            \n",
    "    '''\n",
    "    \n",
    "    url = find_url(url_path, filename)\n",
    "    text = get_html_text(url)\n",
    "    \n",
    "    stripped_text = strip_tags(text).split('\\n')\n",
    "    begin_index = stripped_text.index('                    % Change (24h)Price Graph (7d)')\n",
    "    end_index = stripped_text.index('                        Next 100  →')\n",
    "    indexed_text = stripped_text[begin_index:end_index] \n",
    "\n",
    "    spaceless_text = [item.replace(\" \", \"\") for item in indexed_text]\n",
    "    nostar_text = [item.replace(\"*\", \"\") for item in spaceless_text]\n",
    "    not_empty_text = [item for item in nostar_text if len(item)>0]\n",
    "\n",
    "    raw_data = []\n",
    "    for i in range(1, len(not_empty_text), 8):\n",
    "        raw_data.append(not_empty_text[i:i+8])\n",
    "        \n",
    "    columns = ['rank', \"Name\", \"Market_cap\", \"USD_pric\", \n",
    "               \"Available_supply\", \"symbol\", \"USD_volume_24h\", \"USD_percent_change_24h\"]\n",
    "    \n",
    "    df = list_to_dataframe(raw_data, url, columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_old_site_2017(url_path, filename):\n",
    "    ''' Scrape and parse the old site and return a pandas dataframe\n",
    "    \n",
    "        This funtion is triggered if parsing the JSON file from the \"scrape_json\" could not be parsed.\n",
    "        The function first scrapes the intended page, then removes all the HTML so that \n",
    "        only a list with the data remains. This list is then index and written to a dataframe.\n",
    "        \n",
    "        params:\n",
    "            url_path: string - Directory of the URL list\n",
    "            filename: string - Name of the JSON file that could not be parsed\n",
    "        \n",
    "        returns:\n",
    "            df: pandas dataframe - dataframe with scraped data            \n",
    "    '''\n",
    "    \n",
    "    url = find_url(url_path, filename)\n",
    "    text = get_html_text(url)\n",
    "    stripped_text = strip_tags(text).split('\\n')\n",
    "    spaceless_text = [item.replace(\" \", \"\") for item in stripped_text]\n",
    "    nostar_text = [item.replace(\"*\", \"\") for item in spaceless_text]\n",
    "    not_empty_text = [item for item in nostar_text if len(item)>0]\n",
    "    try:\n",
    "        begin_index = not_empty_text.index('%Change(24h)PriceGraph(7d)')\n",
    "    except:\n",
    "        begin_index = not_empty_text.index('PriceGraph(7d)')\n",
    "\n",
    "    end_index = not_empty_text.index('NotMineable')\n",
    "\n",
    "    indexed_text = not_empty_text[begin_index:end_index-2] \n",
    "\n",
    "    clean_text = text_cleaner(indexed_text)\n",
    "\n",
    "    raw_data = []\n",
    "    for i in range(1, len(clean_text), 9):\n",
    "        raw_data.append(clean_text[i:i+9])\n",
    "        \n",
    "    columns = ['rank', \"Ticker\",\"Name\", \"Market_cap\", \"USD_pric\", \"USD_volume_24h\",\n",
    "               \"Available_supply\", \"symbol\", \"USD_percent_change_24h\"]\n",
    "    \n",
    "    df = list_to_dataframe(raw_data, url, columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv(path):\n",
    "    ''' Open a CSV and return a list with the contents'''\n",
    "\n",
    "    with open(path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        url_list = list(reader)[1:]\n",
    "        \n",
    "    return [url[0] for url in url_list]\n",
    "\n",
    "\n",
    "def save_df(json_path, url_path):\n",
    "    ''' Open JSON json files and call the \"parse_json\" function\n",
    "    \n",
    "        Some JSON files are empty because the old CoinMarketCap website cannot be \n",
    "        scraped in the same way as the new one. For the old site I made a seperate \n",
    "        function that both scrapes and parses the data.\n",
    "        \n",
    "        params:\n",
    "        \n",
    "            json_path: string - Directory of the JSON files\n",
    "            url_path: string - Directory of the URL list\n",
    "        \n",
    "    '''\n",
    "    is_2017 = False\n",
    "    \n",
    "    for file in progressbar(os.listdir(json_path)):\n",
    "        file_path = f\"{json_path}/{file}\"\n",
    "        filename = file.replace(\".json\", \".csv\")\n",
    "        print(filename)\n",
    "        \n",
    "        try:\n",
    "            df = parse_json(file_path)\n",
    "            df_clean = clean_data(df)\n",
    "            df_clean.to_csv(f\"data/market_cap/{filename}\", index=False)\n",
    "\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            \n",
    "            if filename == \"201710.csv\":\n",
    "                is_2017 = True\n",
    "            if is_2017:\n",
    "                df_old_site_2017 = parse_old_site_2017(url_path, file)\n",
    "                df_old_site_2017.to_csv(f\"data/market_cap/{filename}\", index=False)\n",
    "            else: \n",
    "                df_old_site_2016 = parse_old_site_2016(url_path, file)\n",
    "                df_old_site_2016.to_csv(f\"data/market_cap/{filename}\", index=False)\n",
    "        \n",
    "        except AttributeError:\n",
    "            print('NOOO:', filename)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "N/A% (0 of 61) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201612.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "  1% (1 of 61) |                         | Elapsed Time: 0:00:03 ETA:   0:03:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201701.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "  3% (2 of 61) |                         | Elapsed Time: 0:00:06 ETA:   0:03:06"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201702.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "  4% (3 of 61) |#                        | Elapsed Time: 0:00:10 ETA:   0:03:20"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201703.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "  6% (4 of 61) |#                        | Elapsed Time: 0:00:12 ETA:   0:02:04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201704.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "  8% (5 of 61) |##                       | Elapsed Time: 0:00:14 ETA:   0:02:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201705.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      "  9% (6 of 61) |##                       | Elapsed Time: 0:00:16 ETA:   0:01:52"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201706.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 11% (7 of 61) |##                       | Elapsed Time: 0:00:18 ETA:   0:01:46"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201707.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 13% (8 of 61) |###                      | Elapsed Time: 0:00:20 ETA:   0:01:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201708.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 14% (9 of 61) |###                      | Elapsed Time: 0:00:24 ETA:   0:03:39"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201709.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 16% (10 of 61) |###                     | Elapsed Time: 0:00:29 ETA:   0:03:30"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201710.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 18% (11 of 61) |####                    | Elapsed Time: 0:00:32 ETA:   0:02:33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201711.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 19% (12 of 61) |####                    | Elapsed Time: 0:00:34 ETA:   0:02:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201712.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 21% (13 of 61) |#####                   | Elapsed Time: 0:00:36 ETA:   0:01:46"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201801.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 22% (14 of 61) |#####                   | Elapsed Time: 0:00:40 ETA:   0:03:04"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201802.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 24% (15 of 61) |#####                   | Elapsed Time: 0:00:44 ETA:   0:03:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201803.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 26% (16 of 61) |######                  | Elapsed Time: 0:00:48 ETA:   0:02:57"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201804.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 27% (17 of 61) |######                  | Elapsed Time: 0:00:51 ETA:   0:02:28"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201805.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 29% (18 of 61) |#######                 | Elapsed Time: 0:00:54 ETA:   0:01:58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201806.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 31% (19 of 61) |#######                 | Elapsed Time: 0:00:58 ETA:   0:02:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201807.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 32% (20 of 61) |#######                 | Elapsed Time: 0:01:02 ETA:   0:03:16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201808.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 34% (21 of 61) |########                | Elapsed Time: 0:01:05 ETA:   0:01:26"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201809.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 36% (22 of 61) |########                | Elapsed Time: 0:01:11 ETA:   0:03:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201810.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 37% (23 of 61) |#########               | Elapsed Time: 0:01:17 ETA:   0:03:45"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201811.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 39% (24 of 61) |#########               | Elapsed Time: 0:01:19 ETA:   0:01:31"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201812.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 40% (25 of 61) |#########               | Elapsed Time: 0:01:21 ETA:   0:01:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201901.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 42% (26 of 61) |##########              | Elapsed Time: 0:01:25 ETA:   0:01:52"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201902.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 44% (27 of 61) |##########              | Elapsed Time: 0:01:28 ETA:   0:02:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201903.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 45% (28 of 61) |###########             | Elapsed Time: 0:01:32 ETA:   0:01:44"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201904.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 47% (29 of 61) |###########             | Elapsed Time: 0:01:35 ETA:   0:02:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201905.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 49% (30 of 61) |###########             | Elapsed Time: 0:01:38 ETA:   0:01:31"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201906.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 50% (31 of 61) |############            | Elapsed Time: 0:01:43 ETA:   0:02:23"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201907.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 52% (32 of 61) |############            | Elapsed Time: 0:01:47 ETA:   0:01:40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201908.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 54% (33 of 61) |############            | Elapsed Time: 0:01:50 ETA:   0:01:31"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201909.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 55% (34 of 61) |#############           | Elapsed Time: 0:01:53 ETA:   0:01:25"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201910.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                               \r",
      "\r",
      " 57% (35 of 61) |#############           | Elapsed Time: 0:01:57 ETA:   0:01:47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201911.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59% (36 of 61) |##############          | Elapsed Time: 0:02:00 ETA:   0:01:06C:\\Users\\srema\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  \n",
      " 68% (42 of 61) |################        | Elapsed Time: 0:02:00 ETA:   0:00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201912.csv\n",
      "202001.csv\n",
      "202002.csv\n",
      "202003.csv\n",
      "202004.csv\n",
      "202005.csv\n",
      "202006.csv\n",
      "202007.csv\n",
      "202008.csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85% (52 of 61) |####################    | Elapsed Time: 0:02:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "202009.csv\n",
      "202010.csv\n",
      "202011.csv\n",
      "202012.csv\n",
      "202101.csv\n",
      "202102.csv\n",
      "202103.csv\n",
      "202104.csv\n",
      "202105.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (61 of 61) |########################| Elapsed Time: 0:02:00 Time:  0:02:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202106.csv\n",
      "202107.csv\n",
      "202108.csv\n",
      "202109.csv\n",
      "202110.csv\n",
      "NOOO: 202110.csv\n",
      "202111.csv\n",
      "NOOO: 202111.csv\n",
      "202112.csv\n",
      "NOOO: 202112.csv\n"
     ]
    }
   ],
   "source": [
    "url_path = 'data/market_cap/urls.csv'\n",
    "\n",
    "json_path = \"data/json\"\n",
    "# scrape_json(json_path, url_path)\n",
    "save_df(json_path, url_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
