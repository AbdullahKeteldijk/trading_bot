{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srema\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import ftplib\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    from requests_html import HTMLSession\n",
    "except Exception:\n",
    "    print(\"\"\"Warning - Certain functionality \n",
    "             requires requests_html, which is not installed.\n",
    "             \n",
    "             Install using: \n",
    "             pip install requests_html\n",
    "             \n",
    "             After installation, you may have to restart your Python session.\"\"\")\n",
    "    \n",
    "import csv\n",
    "import pandas as pd\n",
    "from progressbar import progressbar\n",
    "\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML cleaner\n",
    "# Source: https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python/925630#925630\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    ''' Strip HTML tags from text'''\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data and saving as JSON (new site)\n",
    "\n",
    "def get_html_text(url):\n",
    "    ''' Send HTML request and return HTML text in UTF-8 format'''\n",
    "    \n",
    "    session = HTMLSession()\n",
    "    resp = session.get(url)\n",
    "    text = resp.html.raw_html.decode(\"utf-8\")  \n",
    "    session.close()   \n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    ''' Remove emojis and special characters'''\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    no_emoji = emoji_pattern.sub(r'', text) \n",
    "    return (no_emoji.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
    "\n",
    "def parse_html(url):\n",
    "    ''' Fetch HTML and clean it  to get raw JSON string'''\n",
    "    \n",
    "    text = get_html_text(url)\n",
    "    text_split = text.split('\\n')\n",
    "    test_str = strip_tags(text_split[-21])\n",
    "    raw_data = test_str[test_str.find('{\"props\"'):]\n",
    "    cleaned_raw_data = remove_emojis(raw_data)\n",
    "    \n",
    "    return cleaned_raw_data\n",
    "\n",
    "def save_json(path, filename, text):\n",
    "    ''' Save scraped text as JSON in UTF-8 format'''\n",
    "    \n",
    "    text_file = codecs.open(f\"{path}/{filename}.json\", \"w\", \"utf-8\")\n",
    "    n = text_file.write(text)\n",
    "    text_file.close()\n",
    "    \n",
    "def get_filename(url):\n",
    "    ''' Get filename from URL'''\n",
    "    year_month = url.split('/')\n",
    "    return year_month[4][:6]\n",
    "    \n",
    "def scrape_json(json_path, url_path):\n",
    "    ''' Scrape URL and save it as JSON'''\n",
    "    \n",
    "    url_list = open_csv(url_path)\n",
    "    \n",
    "    for url in progressbar(url_list):\n",
    "        time.sleep(10)\n",
    "        raw_data = parse_html(url)\n",
    "        filename = get_filename(url)\n",
    "        save_json(json_path, filename, raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "# fuzz.ratio(\"quote_USD_lastUpdated\", \"quote_USD_last_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing data from JSON files (new site)\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    ''' Flatten dictionary'''\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def clean_data(df):\n",
    "    ''' Adds extra colums to dataframe and renames some columns'''\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    df.columns = df.columns.str.strip('quote')\n",
    "    df_clean['Ticker'] = df_clean['symbol'] + '-USD'\n",
    "    df_clean['Date'] = pd.to_datetime(df_clean['quoteusdlastupdated']).dt.date\n",
    "    df_clean = df_clean.rename(columns={\"id\": \"ID\", \"nam\": \"Name\", \"quoteusdmarketcap\": \"Market_cap\"})\n",
    "    \n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# def list_to_string(data):\n",
    "    \n",
    "#     new_data = []\n",
    "    \n",
    "#     for item in data:\n",
    "#         if isinstance(item, list):\n",
    "#             if len(item)>0 and isinstance(item[0], dict):\n",
    "#                 list_string = str(item[0])\n",
    "#                 new_data.append(f\"[{list_string}]\")\n",
    "#             else:\n",
    "#                 join_string = \", \"\n",
    "#                 list_string = join_string.join(item)\n",
    "#                 new_data.append(f\"[{list_string}]\")\n",
    "# #             except Exception as e:\n",
    "# #                 list_string = str(item[0])\n",
    "# #                 print('Item:', list_string)\n",
    "# #                 print(\"\")\n",
    "# #                 print(\"Data\", data)\n",
    "# #                 print('')\n",
    "# #                 print(e)\n",
    "# #                 print('--------------------------------')\n",
    "# #                 new_data.append(f\"[{list_string}]\")\n",
    "#         else:\n",
    "#             new_data.append(item)\n",
    "    \n",
    "#     return new_data\n",
    "    \n",
    "# de rename_fields():\n",
    "    \n",
    "\n",
    "def remove_list_from_dict(dictionary):\n",
    "    ''' Remove columns from dictionary that are not in the list'''\n",
    "    \n",
    "    key_list = ['id', 'name', 'symbol', 'slug', 'cmcrank', 'marketpaircount', 'circulatingsupply',\n",
    "                'totalsupply', 'lastupdated', 'dateadded', 'tags', 'rank',  \n",
    "                'quoteusdname', 'quoteusdprice', 'quoteusdvolume24h', \n",
    "                'quoteusdmarketcap', 'quoteusdpercentchange1h', 'quoteusdpercentchange24h', \n",
    "                'quoteusdpercentchange7d', 'quoteusdlastupdated']\n",
    "    \n",
    "    copy_dict = dictionary.copy()\n",
    "    \n",
    "    for key in dictionary.keys():\n",
    "        if key not in key_list:\n",
    "            del copy_dict[key]\n",
    "    \n",
    "    return copy_dict\n",
    "\n",
    "# def fill_empty(dictionary):\n",
    "    \n",
    "#     for key, value in dictionary.items():\n",
    "#         if dictionary[key] == '':\n",
    "#             dictionary[key] = \"NaN\"\n",
    "        \n",
    "    \n",
    "#     return \n",
    "\n",
    "def format_keys(dictionary):\n",
    "    ''' Convert keys to lowercase and remove underscore'''\n",
    "    \n",
    "    old_keys = dictionary.keys()\n",
    "    new_dict = {}\n",
    "    \n",
    "    for key, value in dictionary.items():\n",
    "        new_key = key.lower().replace(\"_\", \"\")\n",
    "        new_dict[new_key] = value\n",
    "    \n",
    "    return new_dict\n",
    "    \n",
    "def parse_json(file_path):\n",
    "    ''' Parses JSON file and converts it into a dataframe. \n",
    "    \n",
    "        The JSON structure is loaded as a dictionary and then flatted \n",
    "        after which the data parsed into a dataframe\n",
    "        \n",
    "        params: \n",
    "            file_path: string - Directory if JSON files\n",
    "        \n",
    "        returns:\n",
    "            df_clean: pandas - Pandas dataframe with cleaned data\n",
    "    '''\n",
    "    \n",
    "    row_values = []\n",
    "     \n",
    "    col_names = []\n",
    "    \n",
    "    with open(file_path) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "    \n",
    "    json_dict = json_data['props']['initialState']['cryptocurrency']['listingLatest']['data']\n",
    "\n",
    "    for i in range(len(json_dict)):\n",
    "        \n",
    "        flat_dict = format_keys(flatten(json_dict[i], parent_key='', sep='_'))\n",
    "        clean_dict = remove_list_from_dict(flat_dict)\n",
    "        item_list = list(clean_dict.values())\n",
    "\n",
    "        row_values.append(item_list)\n",
    "        \n",
    "        if len(col_names) == 0:\n",
    "            col_names = list(clean_dict.keys())\n",
    "    \n",
    "\n",
    "    df_raw = pd.DataFrame(row_values, columns=col_names)\n",
    "                \n",
    "    df_clean = clean_data(df_raw)\n",
    "    \n",
    "    return df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfs\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "file_path = \"data/playground/202110.json\"\n",
    "try:\n",
    "    parse_json(file_path)\n",
    "except AttributeError:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and parsing the old site\n",
    "\n",
    "def get_date(url):\n",
    "    ''' Return date from URL'''\n",
    "    \n",
    "    split_url = url.split('/')\n",
    "    raw_date = split_url[4][:8]\n",
    "    return f\"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:]}\"\n",
    "    \n",
    "def list_to_dataframe(raw_data, url, columns):\n",
    "    ''' Convert list of lists with scraped data to a dataframe and add some extra columns'''\n",
    "    \n",
    "    df = pd.DataFrame.from_records(raw_data, columns=columns)\n",
    "    df['Ticker'] = df['symbol'] + '-USD'\n",
    "    df['Date'] = get_date(url)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def find_url(path, filename):\n",
    "    ''' Find the URL of the file that could not be parsed by looking for a substring in the URL list'''\n",
    "    \n",
    "    url_list = open_csv(path)\n",
    "    date = filename.replace(\".json\", \"\")\n",
    "    index = [i for i, s in enumerate(url_list) if date in s][0]\n",
    "    \n",
    "    return url_list[index]\n",
    "\n",
    "def text_cleaner(text):\n",
    "    ''' Remove duplicates from text'''\n",
    "    \n",
    "    clean_text = text\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        is_int = False\n",
    "        try:\n",
    "            int(text[i])\n",
    "            is_int = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if is_int:\n",
    "            unique_item = text[i+1]\n",
    "            if unique_item == text[i+6]:\n",
    "                del text[i+6]\n",
    "    \n",
    "    return clean_text\n",
    "    \n",
    "def parse_old_site_2016(url_path, filename):\n",
    "    ''' Scrape and parse the old site and return a pandas dataframe\n",
    "    \n",
    "        This funtion is triggered if parsing the JSON file from the \"scrape_json\" could not be parsed.\n",
    "        The function first scrapes the intended page, then removes all the HTML so that \n",
    "        only a list with the data remains. This list is then index and written to a dataframe.\n",
    "        \n",
    "        params:\n",
    "            url_path: string - Directory of the URL list\n",
    "            filename: string - Name of the JSON file that could not be parsed\n",
    "        \n",
    "        returns:\n",
    "            df: pandas dataframe - dataframe with scraped data            \n",
    "    '''\n",
    "    \n",
    "    url = find_url(url_path, filename)\n",
    "    text = get_html_text(url)\n",
    "    \n",
    "    stripped_text = strip_tags(text).split('\\n')\n",
    "    begin_index = stripped_text.index('                    % Change (24h)Price Graph (7d)')\n",
    "    end_index = stripped_text.index('                        Next 100  â†’')\n",
    "    indexed_text = stripped_text[begin_index:end_index] \n",
    "\n",
    "    spaceless_text = [item.replace(\" \", \"\") for item in indexed_text]\n",
    "    nostar_text = [item.replace(\"*\", \"\") for item in spaceless_text]\n",
    "    not_empty_text = [item for item in nostar_text if len(item)>0]\n",
    "\n",
    "    raw_data = []\n",
    "    for i in range(1, len(not_empty_text), 8):\n",
    "        raw_data.append(not_empty_text[i:i+8])\n",
    "        \n",
    "    columns = ['rank', \"Name\", \"Market_cap\", \"USD_pric\", \n",
    "               \"Available_supply\", \"symbol\", \"USD_volume_24h\", \"USD_percent_change_24h\"]\n",
    "    \n",
    "    df = list_to_dataframe(raw_data, url, columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def parse_old_site_2017(url_path, filename):\n",
    "    ''' Scrape and parse the old site and return a pandas dataframe\n",
    "    \n",
    "        This funtion is triggered if parsing the JSON file from the \"scrape_json\" could not be parsed.\n",
    "        The function first scrapes the intended page, then removes all the HTML so that \n",
    "        only a list with the data remains. This list is then index and written to a dataframe.\n",
    "        \n",
    "        params:\n",
    "            url_path: string - Directory of the URL list\n",
    "            filename: string - Name of the JSON file that could not be parsed\n",
    "        \n",
    "        returns:\n",
    "            df: pandas dataframe - dataframe with scraped data            \n",
    "    '''\n",
    "    \n",
    "    url = find_url(url_path, filename)\n",
    "    text = get_html_text(url)\n",
    "    stripped_text = strip_tags(text).split('\\n')\n",
    "    spaceless_text = [item.replace(\" \", \"\") for item in stripped_text]\n",
    "    nostar_text = [item.replace(\"*\", \"\") for item in spaceless_text]\n",
    "    not_empty_text = [item for item in nostar_text if len(item)>0]\n",
    "    try:\n",
    "        begin_index = not_empty_text.index('%Change(24h)PriceGraph(7d)')\n",
    "    except:\n",
    "        begin_index = not_empty_text.index('PriceGraph(7d)')\n",
    "\n",
    "    end_index = not_empty_text.index('NotMineable')\n",
    "\n",
    "    indexed_text = not_empty_text[begin_index:end_index-2] \n",
    "\n",
    "    clean_text = text_cleaner(indexed_text)\n",
    "\n",
    "    raw_data = []\n",
    "    for i in range(1, len(clean_text), 9):\n",
    "        raw_data.append(clean_text[i:i+9])\n",
    "        \n",
    "    columns = ['rank', \"Ticker\",\"Name\", \"Market_cap\", \"USD_pric\", \"USD_volume_24h\",\n",
    "               \"Available_supply\", \"symbol\", \"USD_percent_change_24h\"]\n",
    "    \n",
    "    df = list_to_dataframe(raw_data, url, columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv(path):\n",
    "    ''' Open a CSV and return a list with the contents'''\n",
    "\n",
    "    with open(path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        url_list = list(reader)[1:]\n",
    "        \n",
    "    return [url[0] for url in url_list]\n",
    "\n",
    "\n",
    "def save_df(json_path, url_path):\n",
    "    ''' Open JSON json files and call the \"parse_json\" function\n",
    "    \n",
    "        Some JSON files are empty because the old CoinMarketCap website cannot be \n",
    "        scraped in the same way as the new one. For the old site I made a seperate \n",
    "        function that both scrapes and parses the data.\n",
    "        \n",
    "        params:\n",
    "        \n",
    "            json_path: string - Directory of the JSON files\n",
    "            url_path: string - Directory of the URL list\n",
    "        \n",
    "    '''\n",
    "    is_2017 = False\n",
    "    \n",
    "    for file in progressbar(os.listdir(json_path)):\n",
    "        file_path = f\"{json_path}/{file}\"\n",
    "        filename = file.replace(\".json\", \".csv\")\n",
    "        print(filename)\n",
    "        \n",
    "        try:\n",
    "            df = parse_json(file_path)\n",
    "            df_clean = clean_data(df)\n",
    "            df_clean.to_csv(f\"data/market_cap/{filename}\", index=False)\n",
    "\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            \n",
    "            if filename == \"201710.csv\":\n",
    "                is_2017 = True\n",
    "            if is_2017:\n",
    "                df_old_site_2017 = parse_old_site_2017(url_path, file)\n",
    "                df_old_site_2017.to_csv(f\"data/market_cap/{filename}\", index=False)\n",
    "            else: \n",
    "                df_old_site_2016 = parse_old_site_2016(url_path, file)\n",
    "                df_old_site_2016.to_csv(f\"data/market_cap/{filename}\", index=False)\n",
    "        \n",
    "        except AttributeError:\n",
    "            print('NOOO:', filename)\n",
    "                \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% (6 of 22) |######                   | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202003.csv\n",
      "202004.csv\n",
      "202005.csv\n",
      "202006.csv\n",
      "202007.csv\n",
      "202008.csv\n",
      "202009.csv\n",
      "202010.csv\n",
      "202011.csv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68% (15 of 22) |################        | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "202012.csv\n",
      "202101.csv\n",
      "202102.csv\n",
      "202103.csv\n",
      "202104.csv\n",
      "202105.csv\n",
      "202106.csv\n",
      "202107.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (22 of 22) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202108.csv\n",
      "202109.csv\n",
      "202110.csv\n",
      "NOOO: 202110.csv\n",
      "202111.csv\n",
      "NOOO: 202111.csv\n",
      "202112.csv\n",
      "NOOO: 202112.csv\n"
     ]
    }
   ],
   "source": [
    "url_path = 'data/market_cap/play.csv'\n",
    "\n",
    "json_path = \"data/playground\"\n",
    "# scrape_json(json_path, url_path)\n",
    "save_df(json_path, url_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De vertaling van de JSON files gaat nog niet helemaal goed. Mogelijk kunnen we een aantal velden preventief verwijderen als \n",
    "# we ze toch niet gaan gebruiken. Daarnaast is het ook belangrijk om ...\n",
    "# Aantal items in de rijen komen niet overeen met het aantal items in de kolommen\n",
    "# gaar\n",
    "# Misschien JSON repareren voordat het geparsed wordt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
